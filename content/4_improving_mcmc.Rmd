---
title: "Improving MCMC"
author: "NIMBLE Development Team"
date: "`r Sys.Date()`"
output: html_document
---

```{css, echo = FALSE}
h1, h4.author, h4.date {
  text-align: center;
}
```

```{r setup, include=FALSE, echo = FALSE}
knitr::opts_chunk$set(echo = TRUE)

knitr_opts <- list(
  message = FALSE,
  warning = FALSE,
  collapse = TRUE,
  comment = "#>",
  fig.width = 7,
  dpi = 300,
  out.width = "700px",
  fig.asp = 1 / 1.618,
  cache = FALSE,
  autodep = TRUE,
  cache.comments = TRUE,
  fig.align = "center",
  echo = TRUE,
  results = 'hide',
  eval = FALSE
)
do.call(knitr::opts_chunk$set, knitr_opts)
```

## Example Depletion Study

Consider data from Scruton et al. (1998) "Pamehac Brook: A case study of the restoration of a Newfoundland, Canada, river impacted by flow diversion for pulpwood transportation". Pamehac Brook is a small tributary of the Exploits River and drains an area of 100 km2. Fixed effort electrofishing occurred in 1990, 1991, 1992, 1996, and then more recently in 2016. In general, at least 4 sweeps were carried out. The first year, 1990 was prior to restoration from logging dams that were removed that year. Eight stations were sampled, two above the former diversion, and six below. In depletion studies, it is critical that each survey, fewer and fewer fish should be caught as the area is depleted.


```{r, eval = TRUE}
library(tidyverse)
remdf <- read.csv("data/removal_data.csv")
catch <- remdf %>% 
  pivot_wider(id_cols = c(Site, Station, Year, Sweep), names_from = Species, 
              values_from = Fish.ID, values_fn = ~ length(.x), values_fill = 0) %>%
              pivot_longer(unique(remdf$Species), names_to = "Species", values_to = "Catch")

catch <- catch %>% group_by(Site, Station, Year, Species) %>%
  arrange(Sweep) %>%
  mutate(nCatch = cumsum(Catch)-Catch[1]) %>%
  ungroup() %>%
  mutate(Station = as.numeric(Station))

ggplot(data = catch %>% filter(Year == 1992, Species %in% c("AS", "BT")), 
    aes(x = Sweep, y = Catch, colour = Species, shape = factor(Station))) +
  geom_point() + 
  geom_line() +
  theme_bw() +
  ylab("Catch") + xlab("Sweep") +
  ggtitle("1992 Electrofishing Survey")
```

A basic model for fish depletion can be written as an unknown fish population $N$, being depleted $n$ times. We can assume that the population is Poisson distributed with a mean $\lambda$ that is gamma distributed. This is called a Poisson-Gamma mixture, and is exactly a negative binomial for a single station. We will assume that each station, $s$, is a random draw from the same mean ($\lambda$).

\[
  \begin{aligned}
    N_s & \sim \mbox{Poisson}(\lambda)\\
    \lambda & \sim \mbox{gamma}(0.1, 0.1)
  \end{aligned}
\]

Each catch ($C_i$), the number of remaining fish are caught with probability $p_i$, that depends on some covariates ($X$).

\[
  \begin{aligned}
    C_i & \sim \mbox{Binomial}(N - \sum_{k<i} C_k, p_i)\\
    \mbox{logit}(p_i) & = \sum \beta_k x_{ik}
  \end{aligned}
\]

For this particular example, we will assume that Atlantic salmon and Brook trout share some information about catchability at each site using a random effect and some basic species specific detectability.

```{r}
library(nimble)
model_code <- nimbleCode({

  b0[1] ~ dnorm(0, sd = 3)
  b0[2] ~ dnorm(0, sd = 3)
  sdre ~ dunif(0, 10)
    
  lambda[1] ~ dgamma(0.1,0.1)
  lambda[2] ~ dgamma(0.1,0.1)

  for( s in 1:nstation ){
    re[s] ~ dnorm(0, sd = sdre) # non-centered parameterization
    N[s,1] ~ dpois(lambda[1])
    N[s,2] ~ dpois(lambda[2])
  }
  
  for( i in 1:nobs ){
    logit(p[i]) <- b0[spp[i]] + re[station[i]]
    caught[i] ~ dbinom(size = N[station[i], spp[i]] - total[i], prob = p[i])
  }
})

dat92 <- catch %>% filter(Year == 1992, Species %in% c("AS", "BT"))
data <- list(caught = dat92$Catch)
constants <- list(nobs = nrow(dat92), total = dat92$nCatch, 
                  nstation = length(unique(dat92$Station)), 
                  station = dat92$Station, spp = as.numeric(factor(dat92$Species)))

maxn <- dat92 %>% 
  group_by(Station, Species) %>% 
  summarize(n = sum(Catch), .groups = "drop") %>%
  pivot_wider(names_from = Species, values_from = n)

inits <- function(){
  list(N = as.matrix(maxn[,c("AS", "BT")] + 10, ncol = 2), 
       lambda = c(mean(maxn$AS+10), mean(maxn$BT+10)), b0 = rnorm(2,0,1), 
       re = rnorm(constants$nstation, 0, 1), sdre = runif(1,0.5,2))
}
Rmodel <- nimbleModel(model_code, data = data, constants = constants, inits = inits())

conf <- configureMCMC(Rmodel, monitors = c("b0", "sdre", "lambda", "N", "p"))
Rmcmc <- buildMCMC(conf)
Cmodel <- compileNimble(Rmodel)
Cmcmc <- compileNimble(Rmcmc, project = Rmodel)
samples <- runMCMC(Cmcmc, niter = 10000, nburnin = 2000, nchains = 3, inits = inits(), samplesAsCodaMCMC = TRUE)
plot(samples[,'sdre'])
plot(samples[,'b0[1]'])
plot(samples[,'b0[2]'])
plot(samples[,'lambda[1]'])
plot(samples[,'lambda[2]'])
```

Running 10,000 iterations for many analyses is not enough to be confident that we are fully sampling the posterior distribution. We have options to increase the number of samples, or to explore better samplers that explore posterior more effectively per iteration, although potentially at the cost of computation speed. Check out `help(samplers)` to help choose.

```{r}
sumdf <- data.frame(do.call("cbind", summary(samples)))
sumdf$parameter = gsub("\\[.*", "", rownames(sumdf))
sumdf$idx = gsub("*.\\[|\\]", "", rownames(sumdf))
sumdf$Station = factor(gsub(",.*", "", sumdf$idx))
sumdf$Species = c("AS", "BT")[as.numeric(gsub("*.,", "", sumdf$idx))]
sumdf <- sumdf %>% 
  mutate(Species = ifelse(idx == "lambd1", "AS", Species)) %>%
  mutate(Species = ifelse(idx == "lambd2", "BT", Species))

ggplot(data = sumdf %>% filter(parameter == "N"), aes(x = Station, y = Mean, colour = Species)) +
  geom_point(position = position_dodge(width = 0.25)) + 
  theme_bw() +
  geom_errorbar(aes(ymin = X2.5., ymax = X97.5., colour = Species), 
    position = position_dodge(width = 0.25), width = 0.1) +
  geom_hline(data = sumdf %>% filter(parameter == "lambda"), 
    aes(yintercept = Mean, colour= Species), linetype='dashed')

pout <- sumdf %>% filter(parameter == "p")
datp <- cbind(dat92, pout[, 1:10])
ggplot(data = datp, aes(x = Station, y = Mean, colour = Species)) +
  geom_point(position = position_dodge(width = 0.25)) + 
  theme_bw() +
  geom_errorbar(aes(ymin = X2.5., ymax = X97.5., colour = Species), 
    position = position_dodge(width = 0.25), width = 0.1) 
```

It is likely we can assume that catchability between brook trout and Atlantic salmon is constant, but both vary between sites. Now we will explore ways to improve the mixing of the MCMC.

## Some strategies for improving MCMC

 - Customize sampler choices. E.g.,
    - Try sampling standard deviations on a log scale
    - Try slice samplers instead of Metropolis-Hastings
    - Try blocking correlated parameters
    - Try multiple samplers for slow-mixing parameters [not shown].
 - Reparameterize
    - Center covariates
    - Centered versus non-centered random effects parameterization
    - Rotated coordinates to reduce posterior correlation
 - Rewrite the model. E.g.,
    - Rewrite the model to reduce dependencies
    - Vectorize declarations to improve computational efficiency
    - Marginalize to remove parameters
 - (Advanced) Write new samplers that take advantage of particular model structures


We will start to look at which parameters are correlated and try blocking to improve mixing. Note that blocking is a good strategy to reduce computation time as well as improve mixing. Each iteration of the MCMC requires fewer computation steps when we sample in block. For instance in this example, when we sample `b0[1]` and `lambda[1]` separately, we change first change `b0[1]`, compute the density and the accept/reject, and then change `lambda[1]`, compute the density and accept/reject. By combining these into a block, we change them both at the same time, then compute the density and accept/reject.

***Discussion*** Would we get a speed up if we block `b0[1]` and `b0[2]`?

```{r}
## Subset for key parameters
library(coda)
ids <- grepl("lambda|b0|sdre", colnames(samples[[1]]))
out <- as.mcmc.list(lapply(samples, FUN = function(x){as.mcmc(x[, ids])}))
crosscorr.plot(out)
```

It appears as if the detection parameter `b` and expected number of animals `lambda` are correlated. We will try block sampling them via a random walk. This will be faster but potentially worse mixing.

```{r}
conf$removeSamplers(c("b0", "lambda"))
conf$addSampler(c("b0[1]", "lambda[1]"), type = "RW_block")
conf$addSampler(c("lambda[2]", "b0[2]"), type = "RW_block")
Rmcmc <- buildMCMC(conf)
Cmodel <- compileNimble(Rmodel)
Cmcmc <- compileNimble(Rmcmc, project = Rmodel)
samples.rw <- runMCMC(Cmcmc, niter = 10000, nburnin = 2000, nchains = 3, inits = inits(), samplesAsCodaMCMC = TRUE)
plot(samples.rw[, "b0[1]"])
plot(samples.rw[, "lambda[1]"])
```

***Discussion*** What can we marginalize in this model?

## Sampler choices 

- Sampling standard deviations on the  log scale can help, especially when there is posterior support near 0.
- Slice sampling can help mix a parameter at some computational cost.
- Hamiltonian Monte Carlo (HMC) can help mix blocks of parameters (often all parameters at once) but at heavy computational cost.
- Blocking can help when parameters are correlated *given other parameters*.
    - If parameters are *marginally correlated* but *conditionally independent*, blocking will not help.
    - This occurs if parameters are marginally correlated only because they both depend on something else.
- Model-specific understanding can yield good sampling strategies.

Let's try a slice sampler on these in a block. This will be slower but better mixing. We will also sample `sdre` on the log scale.

```{r}
conf$removeSamplers(c("b0", "lambda", "sdre"))
conf$addSampler(c("b0[1]", "lambda[1]"), type = "AF_slice")
conf$addSampler(c("b0[2]", "lambda[2]"), type = "AF_slice")
conf$addSampler("sdre", type = "RW", control = list(log = TRUE))
Rmcmc <- buildMCMC(conf)
Cmodel <- compileNimble(Rmodel)
Cmcmc <- compileNimble(Rmcmc, project = Rmodel)
samples.sl <- runMCMC(Cmcmc, niter = 10000, nburnin = 2000, nchains = 3, inits = inits(), samplesAsCodaMCMC = TRUE)
plot(samples.sl[, "b0[1]"])
plot(samples.sl[, "lambda[1]"])
plot(samples.sl[, "sdre"])

ids <- grepl("lambda|b0|sdre", colnames(samples[[1]]))
out <- as.mcmc.list(lapply(samples.sl, FUN = function(x){as.mcmc(x[, ids])}))
crosscorr.plot(out)
```

***Discussion*** Can we try HMC on this model?

## Centering covariates or random effects

Centering refers to two issues:

- Centering of covariates as they are provided for the analysis.
    - Think of $y_i = \beta_0 + \beta_1 x_i + \epsilon_i$. 
    - If the $x_i$ are not centered, then considering $\beta_1 \rightarrow \beta_1'$ is also like adding $(\beta_1' - \beta_1) \bar{x}$ to the intercept.
    - A standard result in linear regression is that estimates of $\beta_0$ and $\beta_1$ are correlated.
    - Centering $x_i$ around its mean removes the posterior correlation between $\beta_0$ and $\beta_1$.

- Random effects with a mean of zero (non-centered parameterization) versus centered around a mean (centered parameterization).
    - E.g., `random_effect ~ N(0, sd)` vs. `random_effect ~ N(mean, sd)`.
    - Theory shows either parameterization can be better, depending on the model and data, but with reasonable amount of data, centered is often better.
    - However, for HMC, uncentered is generally better!


```{r}
model_code_centered <- nimbleCode({

  b0[1] ~ dnorm(0, sd = 3)
  b0[2] ~ dnorm(0, sd = 3)
  sdre ~ dunif(0, 10)
    
  lambda[1] ~ dgamma(0.1,0.1)
  lambda[2] ~ dgamma(0.1,0.1)

  for( s in 1:nstation ){
    logitp[s,1] ~ dnorm(b0[1], sd = sdre) # centered parameterization
    logitp[s,2] ~ dnorm(b0[2], sd = sdre)
    N[s,1] ~ dpois(lambda[1])
    N[s,2] ~ dpois(lambda[2])
  }
  
  for( i in 1:nobs ){
    logit(p[i]) <- logitp[station[i], spp[i]]
    caught[i] ~ dbinom(size = N[station[i], spp[i]] - total[i], prob = p[i])
  }
})
```